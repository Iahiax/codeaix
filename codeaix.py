import requests
import random
import threading
import time
import psutil
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes
import logging
import re
import json
from geopy.geocoders import Nominatim
from urllib.parse import urlparse
import os
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from itertools import cycle
from bs4 import BeautifulSoup  # Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
import optuna  # Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

# **1. Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ ÙˆØ§Ù„ÙˆÙƒÙ„Ø§Ø¡**
USERNAME = os.environ.get('PROXY_USERNAME', 'Yahia_FwOsV')
PASSWORD = os.environ.get('PROXY_PASSWORD', 'Yahia+14118482')
COUNTRY = 'US'
def create_proxy_url(username, password, country):
    return f'http://customer-{username}-cc-{country}:{password}@pr.oxylabs.io:7777'
PROXY_URL = create_proxy_url(USERNAME, PASSWORD, COUNTRY)

TELEGRAM_TOKEN = os.environ.get('TELEGRAM_TOKEN', '8121636489:AAGMJcwUhHi-Bk0TDhSrIvASNPaSoF4XZjk')

# **2. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©**
DEFAULT_CONFIG = {
    "DEFAULT_VISITS_PER_DAY": 7000,
    "DEFAULT_CONCURRENCY": 75,
    "MAX_CONCURRENCY": 250,
    "MIN_CONCURRENCY": 15,
    "MAX_RETRIES": 5,
    "PROXY_ROTATION_ENABLED": True,
    "LOG_LEVEL": "INFO",
    "TARGET_URL": "https://ip.oxylabs.io/location",
    "USER_AGENT_ROTATION_ENABLED": True,
    "SMART_ERROR_HANDLING": True,
    "AI_ENABLED": True,
    "AI_ADJUST_CONCURRENCY": True,
    "AI_ADJUST_VISITS": True,
    "AI_MONITOR_INTERVAL": 300,
    "SUCCESS_RATE_THRESHOLD": 0.85,
    "FAILURE_RATE_THRESHOLD": 0.15,
    "RESPONSE_TIME_THRESHOLD": 4,
    "BOUNCE_RATE_THRESHOLD": 0.4,
    "PROXY_ROTATION_FREQUENCY": 0.7,
    "CONCURRENCY_ADJUSTMENT_FACTOR": 0.1,
    "VISITS_ADJUSTMENT_FACTOR": 0.05,
    "PROXY_VALIDATION_THRESHOLD": 0.8,
    "DATA_COLLECTION_ENABLED": True,
    "DATA_COLLECTION_INTERVAL": 3600,
    "ANONYMOUS_PROXIES_ENABLED": True,
    "OPTIMIZE_MODEL_ENABLED": True, # ØªÙØ¹ÙŠÙ„/ØªØ¹Ø·ÙŠÙ„ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
    "NUM_TRIALS": 5, # Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
}

# **3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù…Ù† Ù…Ù„Ù**
def load_config(filename="config.json"):
    try:
        with open(filename, "r") as f:
            config = json.load(f)
            return {**DEFAULT_CONFIG, **config}
    except FileNotFoundError:
        print("Config file not found, using default config.")
        return DEFAULT_CONFIG

CONFIG = load_config()

# **4. Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©**
visits_count = 0
successful_visits = 0
failed_visits = 0
running = False
bot_running = False
lock = threading.Lock()
target_url = CONFIG["TARGET_URL"]
current_concurrency = CONFIG["DEFAULT_CONCURRENCY"]
current_visits_per_day = CONFIG["DEFAULT_VISITS_PER_DAY"]
proxy_rotation_enabled = CONFIG["PROXY_ROTATION_ENABLED"]
smart_error_handling = CONFIG["SMART_ERROR_HANDLING"]
ai_enabled = CONFIG["AI_ENABLED"]
ai_adjust_concurrency = CONFIG["AI_ADJUST_CONCURRENCY"]
ai_adjust_visits = CONFIG["AI_ADJUST_VISITS"]
ai_monitor_interval = CONFIG["AI_MONITOR_INTERVAL"]
success_rate_threshold = CONFIG["SUCCESS_RATE_THRESHOLD"]
failure_rate_threshold = CONFIG["FAILURE_RATE_THRESHOLD"]
response_time_threshold = CONFIG["RESPONSE_TIME_THRESHOLD"]
bounce_rate_threshold = CONFIG["BOUNCE_RATE_THRESHOLD"]
proxy_rotation_frequency = CONFIG["PROXY_ROTATION_FREQUENCY"]
concurrency_adjustment_factor = CONFIG["CONCURRENCY_ADJUSTMENT_FACTOR"]
visits_adjustment_factor = CONFIG["VISITS_ADJUSTMENT_FACTOR"]
proxy_validation_threshold = CONFIG["PROXY_VALIDATION_THRESHOLD"]
data_collection_enabled = CONFIG["DATA_COLLECTION_ENABLED"]
data_collection_interval = CONFIG["DATA_COLLECTION_INTERVAL"]
anonymous_proxies_enabled = CONFIG["ANONYMOUS_PROXIES_ENABLED"]
optimize_model_enabled = CONFIG["OPTIMIZE_MODEL_ENABLED"]
num_trials = CONFIG["NUM_TRIALS"]

# **5. Ù‚Ø§Ø¦Ù…Ø© ÙˆÙƒÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…**
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59",
]
user_agent_cycle = cycle(user_agents) # ØªØ¯ÙˆÙŠØ± ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…

# **6. Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡**
proxy_list = [
    PROXY_URL,
]
current_proxy_index = 0
proxy_cycle = cycle(proxy_list) # ØªØ¯ÙˆÙŠØ± Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡

# **7. Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù‡Ø§Ù… (Multi-Task Learning)**
class MultiTaskModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_tasks):
        super(MultiTaskModel, self).__init__()
        self.shared_layer = nn.Linear(input_size, hidden_size)
        self.task_layers = nn.ModuleList([nn.Linear(hidden_size, 1) for _ in range(num_tasks)])

    def forward(self, x):
        x = torch.relu(self.shared_layer(x))
        outputs = [torch.sigmoid(task_layers(x)) for task_layers in self.task_layers]
        return outputs

# **8. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù‡Ø§Ù…**
input_size = 10  # Ø­Ø¬Ù… Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª (ØªÙ… ØªÙˆØ³ÙŠØ¹Ù‡)
hidden_size = 32  # Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø®ÙÙŠØ©
num_tasks = 2  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‡Ø§Ù… (Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ØŒ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ÙˆÙƒÙŠÙ„)
model = MultiTaskModel(input_size, hidden_size, num_tasks)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# **9. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³Ø¬Ù„**
log_level = getattr(logging, CONFIG["LOG_LEVEL"].upper(), logging.INFO)
logging.basicConfig(filename='bot.log', level=log_level,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# **10. Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ù…ÙˆØ³Ø¹Ø©)**
error_data = [] # ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¦Ù…Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨

# **11. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬**
def prepare_data(data):
    features = []
    labels_error = []
    labels_proxy = []
    for item in data:
        feature = [
            1 if item["user_agent"] in user_agents else 0,
            1 if item["proxy_used"] else 0,
            item["status_code"],
            item["response_time"],
            item["load_time"],
            item["page_size"],
            item["num_requests"],
            1 if item["page_type"] == "product" else 0,
            1 if item["page_type"] == "blog" else 0,
            1 if item["page_type"] == "login" else 0,
        ]
        label_error = 1 if item["error_type"] != "success" else 0
        label_proxy = 1 if item["proxy_valid"] else 0
        features.append(feature)
        labels_error.append(label_error)
        labels_proxy.append(label_proxy)
    return torch.tensor(features, dtype=torch.float32), torch.tensor(labels_error, dtype=torch.float32), torch.tensor(labels_proxy, dtype=torch.float32)

# **12. ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù‡Ø§Ù…**
def train_model(model, optimizer, features, labels_error, labels_proxy, epochs=100):
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(features)
        loss_error = nn.BCELoss()(outputs[0].squeeze(), labels_error)
        loss_proxy = nn.BCELoss()(outputs[1].squeeze(), labels_proxy)
        loss = loss_error + loss_proxy
        loss.backward()
        optimizer.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# **13. Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø±Ø§Ø¨Ø·**
def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

# **14. Ø¯Ø§Ù„Ø© Ù„ØªØ¯ÙˆÙŠØ± Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡**
def rotate_proxy():
    global current_proxy_index
    with lock:
        current_proxy_index = (current_proxy_index + 1) % len(proxy_list)
        return proxy_list[current_proxy_index]

# **15. Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ÙˆÙƒÙŠÙ„ (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬)**
def validate_proxy(proxy, user_agent, status_code, response_time, load_time, page_size, num_requests, page_type):
    model.eval()
    feature = torch.tensor([1 if user_agent in user_agents else 0, 1, status_code, response_time, load_time, page_size, num_requests, 1 if page_type == "product" else 0, 1 if page_type == "blog" else 0, 1 if page_type == "login" else 0], dtype=torch.float32).unsqueeze(0)
    with torch.no_grad():
        outputs = model(feature)
        proxy_valid_prob = outputs[1].item()
    return proxy_valid_prob > CONFIG["PROXY_VALIDATION_THRESHOLD"]

# **16. Ø¯Ø§Ù„Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ Ù…Ù† Ø¹Ù†ÙˆØ§Ù† IP**
def get_location_from_ip(ip_address):
    try:
        geolocator = Nominatim(user_agent="visit_bot")
        location = geolocator.geocode(ip_address)
        if location:
            return location.latitude, location.longitude
        else:
            return None
    except:
        return None

# **17. Ø¯Ø§Ù„Ø© Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø²ÙŠØ§Ø±Ø© (Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ)**
def simulate_visit(retry_count=0):
    global visits_count, successful_visits, failed_visits

    # ØªØ¯ÙˆÙŠØ± Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¨Ø´ÙƒÙ„ Ø§Ø­ØªÙ…Ø§Ù„ÙŠ
    if proxy_rotation_enabled and random.random() < proxy_rotation_frequency:
        current_proxy = next(proxy_cycle)
    else:
        current_proxy = PROXY_URL

    proxies = {
        'http': current_proxy,
        'https': current_proxy,
    }

    # ØªØ¯ÙˆÙŠØ± ÙˆÙƒÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    current_user_agent = next(user_agent_cycle)
    headers = {
        'User-Agent': current_user_agent,
    }

    start_time = time.time()
    try:
        time.sleep(random.uniform(0.5, 2.5))
        response = requests.get(target_url, proxies=proxies, headers=headers, timeout=10)
        end_time = time.time()
        response_time = end_time - start_time

        # ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
        soup = BeautifulSoup(response.content, 'html.parser')
        page_size = len(response.content)
        num_requests = len(soup.find_all())  # ØªÙ‚Ø¯ÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ±

        with lock:
            visits_count += 1
            if response.status_code == 200:
                successful_visits += 1
                logging.info(f"Successful visit to {target_url} with proxy {current_proxy}, response time: {response_time:.2f}s")
                # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
                if data_collection_enabled:
                    page_type = "unknown"  # ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø© Ù‡Ù†Ø§
                    proxy_valid = True
                    new_data = {"error_type": "success", "user_agent": current_user_agent, "proxy_used": True, "status_code": response.status_code, "response_time": response_time, "proxy_valid": proxy_valid, "page_type": page_type, "load_time": response_time * 1000, "page_size": page_size, "num_requests": num_requests}
                    error_data.append(new_data)
            else:
                failed_visits += 1
                log_error(f"HTTP error: {response.status_code} for URL: {target_url} with proxy {current_proxy}, response time: {response_time:.2f}s")
                # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
                if data_collection_enabled:
                    page_type = "unknown"  # ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø© Ù‡Ù†Ø§
                    proxy_valid = False
                    new_data = {"error_type": "http_error", "user_agent": current_user_agent, "proxy_used": True, "status_code": response.status_code, "response_time": response_time, "proxy_valid": proxy_valid, "page_type": page_type, "load_time": response_time * 1000, "page_size": page_size, "num_requests": num_requests}
                    error_data.append(new_data)

                if ai_enabled and smart_error_handling:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø®Ø·Ø£
                    features = torch.tensor([1 if headers['User-Agent'] in user_agents else 0, 1, response.status_code, response_time, 0, 0, 0, 0, 0, 0], dtype=torch.float32).unsqueeze(0)
                    model.eval()
                    with torch.no_grad():
                        outputs = model(features)
                        error_prob = outputs[0].item()
                    if error_prob > 0.5:  # Ø¹ØªØ¨Ø© Ø§Ù„Ø®Ø·Ø£
                        print("Error predicted by model, adjusting parameters...")
                        if ai_adjust_concurrency:
                            adjust_concurrency()
    except requests.exceptions.ProxyError as e:
        end_time = time.time()
        response_time = end_time - start_time
        with lock:
            failed_visits += 1
        log_error(f"Proxy error: {e} for URL: {target_url} with proxy {current_proxy}, response time: {response_time:.2f}s")
        # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        if data_collection_enabled:
            page_type = "unknown"  # ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø© Ù‡Ù†Ø§
            proxy_valid = False
            new_data = {"error_type": "proxy_error", "user_agent": current_user_agent, "proxy_used": True, "status_code": 0, "response_time": response_time, "proxy_valid": proxy_valid, "page_type": page_type, "load_time": response_time * 1000, "page_size": 0, "num_requests": 0}
            error_data.append(new_data)
    except requests.exceptions.RequestException as e:
        end_time = time.time()
        response_time = end_time - start_time
        with lock:
            failed_visits += 1
        log_error(f"Request Exception: {e} for URL: {target_url} with proxy {current_proxy}, response time: {response_time:.2f}s")
        # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        if data_collection_enabled:
            page_type = "unknown"  # ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø© Ù‡Ù†Ø§
            proxy_valid = False
            new_data = {"error_type": "request_exception", "user_agent": current_user_agent, "proxy_used": True, "status_code": 0, "response_time": response_time, "proxy_valid": proxy_valid, "page_type": page_type, "load_time": response_time * 1000, "page_size": 0, "num_requests": 0}
            error_data.append(new_data)

# **18. Ø¯Ø§Ù„Ø© Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡**
def log_error(message):
    logging.error(message)
    print(f"Error: {message}")

# **19. Ø¯Ø§Ù„Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªØ²Ø§Ù…Ù†**
def run_visits(concurrency, visits_per_day, stop_event):
    global running
    running = True
    visits_per_minute = visits_per_day / (24 * 60)
    interval = 60 / visits_per_minute
    while running and not stop_event.is_set():
        threads = []
        for _ in range(concurrency):
            t = threading.Thread(target=simulate_visit)
            threads.append(t)
            t.start()
        for t in threads:
            t.join()
        time.sleep(interval)

# **20. Ø¯Ø§Ù„Ø© Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØ²Ø§Ù…Ù† (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ)**
def adjust_concurrency():
    global current_concurrency, current_visits_per_day
    cpu_usage = psutil.cpu_percent(interval=1)
    mem_usage = psutil.virtual_memory().percent
    print(f"CPU Usage: {cpu_usage}%, Memory Usage: {mem_usage}%")
    # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø·Ù‚ Ù‡Ù†Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    if cpu_usage > 80 or mem_usage > 80:
        current_concurrency = max(CONFIG["MIN_CONCURRENCY"], int(current_concurrency * (1 - concurrency_adjustment_factor)))
        print(f"Reducing concurrency to {current_concurrency} due to high resource usage.")
    else:
        if current_concurrency < CONFIG["DEFAULT_CONCURRENCY"]:
            current_concurrency = min(CONFIG["DEFAULT_CONCURRENCY"], int(current_concurrency * (1 + concurrency_adjustment_factor)))
            print(f"Increasing concurrency to {current_concurrency}.")
        elif current_concurrency < CONFIG["MAX_CONCURRENCY"]:
            current_concurrency = min(CONFIG["MAX_CONCURRENCY"], int(current_concurrency * (1 + concurrency_adjustment_factor)))
            print(f"Increasing concurrency to {current_concurrency}.")
    return current_concurrency

# **21. Ø¯Ø§Ù„Ø© Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ© (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ)**
def adjust_visits_per_day():
    global current_visits_per_day
    # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø·Ù‚ Ù‡Ù†Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    # Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ Ù…Ø±ØªÙØ¹Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ† Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª
    # ÙˆØ¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙØ´Ù„ Ù…Ø±ØªÙØ¹Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ† ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª
    if successful_visits > visits_count * success_rate_threshold:
        current_visits_per_day = min(int(current_visits_per_day * (1 + visits_adjustment_factor)), 15000)  # Ø²ÙŠØ§Ø¯Ø© Ø¨Ù†Ø³Ø¨Ø© 5%
        print(f"Increasing visits per day to {current_visits_per_day}.")
    elif failed_visits > visits_count * failure_rate_threshold:
        current_visits_per_day = max(int(current_visits_per_day * (1 - visits_adjustment_factor)), 2000)  # ØªÙ‚Ù„ÙŠÙ„ Ø¨Ù†Ø³Ø¨Ø© 5%
        print(f"Reducing visits per day to {current_visits_per_day}.")
    return current_visits_per_day

# **22. Ø¯Ø§Ù„Ø© Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª**
def monitor_performance():
    global successful_visits, failed_visits, visits_count, current_concurrency, current_visits_per_day

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
    success_rate = successful_visits / visits_count if visits_count > 0 else 0
    failure_rate = failed_visits / visits_count if visits_count > 0 else 0
    # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ù‡Ù†Ø§ Ù…Ø«Ù„ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙˆÙ…Ø¹Ø¯Ù„ Ø§Ù„Ø§Ø±ØªØ¯Ø§Ø¯

    print(f"Success Rate: {success_rate:.2f}, Failure Rate: {failure_rate:.2f}")

    # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª
    if success_rate < success_rate_threshold:
        print("Low success rate, adjusting parameters...")
        # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø·Ù‚ Ù‡Ù†Ø§ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª
        # Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ† Ø²ÙŠØ§Ø¯Ø© ØªØ¯ÙˆÙŠØ± Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø£Ùˆ ØªØºÙŠÙŠØ± User-Agent
        if proxy_rotation_enabled:
            print("Increasing proxy rotation frequency...")
            global proxy_rotation_frequency
            proxy_rotation_frequency = min(proxy_rotation_frequency + 0.1, 1.0)  # Ø²ÙŠØ§Ø¯Ø© Ø¨Ù†Ø³Ø¨Ø© 10%
        else:
            print("Enabling proxy rotation...")
            global proxy_rotation_enabled
            proxy_rotation_enabled = True

    if failure_rate > failure_rate_threshold:
        print("High failure rate, adjusting parameters...")
        # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø·Ù‚ Ù‡Ù†Ø§ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª
        # Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ† ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ²Ø§Ù…Ù† Ø£Ùˆ ØªØºÙŠÙŠØ± Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡
        current_concurrency = max(CONFIG["MIN_CONCURRENCY"], int(current_concurrency * (1 - concurrency_adjustment_factor)))
        print(f"Reducing concurrency to {current_concurrency}.")

    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª
    with lock:
        successful_visits = 0
        failed_visits = 0
        visits_count = 0

# **23. Ø¯Ø§Ù„Ø© Ù„Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§**
def collect_data():
    global error_data
    print("Collecting data...")
    # ÙŠÙ…ÙƒÙ†Ùƒ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¨ÙˆØª Ù‡Ù†Ø§
    # Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„
    # Ø£Ùˆ ÙŠÙ…ÙƒÙ†Ùƒ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø®Ù„Ø§Ù„ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¨ÙˆØª
    # ÙŠØ¬Ø¨ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙƒØ§Ù…Ù„Ø© ÙˆØªÙ…Ø«Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª
    # ÙŠØ¬Ø¨ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ù…ØªÙˆØ§Ø²Ù†Ø©
    # ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø«Ù„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ (resampling) Ø£Ùˆ ØªØ±Ø¬ÙŠØ­ Ø§Ù„ÙØ¦Ø§Øª (class weighting)
    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆØ§Ø²Ù†Ø©
    print("Data collection complete.")

# **24. Ø¯Ø§Ù„Ø© Ù„ØªØ­Ø³ÙŠÙ† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù‡Ø§Ù…**
def optimize_model():
    global model, optimizer, features, labels_error, labels_proxy
    print("Optimizing model...")

    # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    features, labels_error, labels_proxy = prepare_data(error_data)

    # 2. ØªØ­Ø¯ÙŠØ¯ Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª
    param_grid = {
        'hidden_size': [16, 32, 64],
        'learning_rate': [0.001, 0.01, 0.1],
        'epochs': [50, 100, 150]
    }

    # 3. ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„Ù‡Ø¯Ù
    def objective(trial):
        hidden_size = trial.suggest_categorical('hidden_size', [16, 32, 64])
        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)
        epochs = trial.suggest_int('epochs', 50, 150)

        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model = MultiTaskModel(input_size, hidden_size, num_tasks)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)

        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        train_model(model, optimizer, features, labels_error, labels_proxy, epochs)

        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model.eval()
        with torch.no_grad():
            outputs = model(features)
            loss_error = nn.BCELoss()(outputs[0].squeeze(), labels_error)
            loss_proxy = nn.BCELoss()(outputs[1].squeeze(), labels_proxy)
            loss = loss_error + loss_proxy

        return loss.item()

    # 4. ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†
    import optuna
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=num_trials)

    # 5. Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª
    best_params = study.best_params
    print(f"Best parameters: {best_params}")

    # 6. Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª
    model = MultiTaskModel(input_size, best_params['hidden_size'], num_tasks)
    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])

    # 7. Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª
    train_model(model, optimizer, features, labels_error, labels_proxy, best_params['epochs'])

    print("Model optimization complete.")

# **25. Ø¯Ø§Ù„Ø© Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…**
def monitor_resources(stop_event):
    while running and not stop_event.is_set():
        cpu_usage = psutil.cpu_percent(interval=1)
        mem_usage = psutil.virtual_memory().percent
        print(f"Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬: {cpu_usage}%, Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {mem_usage}%")
        if cpu_usage > 80 or mem_usage > 80:
            adjust_concurrency()
        if ai_enabled and ai_adjust_visits:
            adjust_visits_per_day()
        time.sleep(60)

# **26. Ø¯Ø§Ù„Ø© Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¯ÙˆØ±ÙŠØ©**
def ai_monitor(stop_event):
    while running and not stop_event.is_set():
        time.sleep(ai_monitor_interval)
        monitor_performance()

# **27. Ø¯Ø§Ù„Ø© Ù„Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ**
def data_collection_loop(stop_event):
    while running and not stop_event.is_set():
        time.sleep(data_collection_interval)
        if data_collection_enabled:
            collect_data()
            if optimize_model_enabled:
                optimize_model() # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

# **28. Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø£ÙˆØ§Ù…Ø± (Ù…Ø­Ø¯Ø«Ø© Ø¥Ù„Ù‰ async)**
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global running, bot_running
    with lock:
        if not bot_running:
            bot_running = True
            await update.message.reply_text('ØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª Ø¨Ù†Ø¬Ø§Ø­ ÙˆÙ„ÙƒÙ† Ù„Ù… ÙŠØ¨Ø¯Ø£ Ø£ÙŠ Ø²ÙŠØ§Ø±Ø§Øª Ø¨Ø¹Ø¯.')
        else:
            await update.message.reply_text('Ø§Ù„Ø¨ÙˆØª ÙŠØ¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„.')

async def startvisits(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global running, target_url, bot_running, stop_event
    if context.args:
        target_url = context.args[0]
        if not is_valid_url(target_url):
            await update.message.reply_text('Ø§Ù„Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø§Ø¨Ø· ØµØ­ÙŠØ­.')
            return
        with lock:
            if not bot_running:
                bot_running = True
                running = True
                concurrency = adjust_concurrency()
                stop_event = threading.Event()
                threading.Thread(target=run_visits, args=(concurrency, current_visits_per_day, stop_event), daemon=True).start()
                threading.Thread(target=monitor_resources, args=(stop_event,), daemon=True).start()
                # Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
                threading.Thread(target=ai_monitor, args=(stop_event,), daemon=True).start()
                # Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ
                threading.Thread(target=data_collection_loop, args=(stop_event,), daemon=True).start()
                await update.message.reply_text(f'Ø¨Ø¯Ø£Øª Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰: {target_url}')
            else:
                if not running:
                    running = True
                    concurrency = adjust_concurrency()
                    stop_event = threading.Event()
                    threading.Thread(target=run_visits, args=(concurrency, current_visits_per_day, stop_event), daemon=True).start()
                    threading.Thread(target=monitor_resources, args=(stop_event,), daemon=True).start()
                    # Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
                    threading.Thread(target=ai_monitor, args=(stop_event,), daemon=True).start()
                    # Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ
                    threading.Thread(target=data_collection_loop, args=(stop_event,), daemon=True).start()
                    await update.message.reply_text(f'Ø¨Ø¯Ø£Øª Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰: {target_url}')
                else:
                    await update.message.reply_text(f'Ø§Ù„Ø¨ÙˆØª ÙŠÙ‚ÙˆÙ… Ø¨Ø²ÙŠØ§Ø±Ø© Ø¨Ø§Ù„ÙØ¹Ù„: {target_url}. ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ù„Ù‰: {target_url}')
    else:
        await update.message.reply_text('Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙ‚Ø¯ÙŠÙ… Ø±Ø§Ø¨Ø·. Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: /startvisits <url>')

async def stop(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global running, bot_running, stop_event
    with lock:
        running = False
        bot_running = False
        if stop_event:
            stop_event.set()
    await update.message.reply_text('ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.')

async def stats(update: Update, context: ContextTypes.DEFAULT_TYPE):
    with lock:
        await update.message.reply_text(f'Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª: {visits_count}\nØ§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {successful_visits}\nØ§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©: {failed_visits}')

async def errors(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text('ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø¨Ø¹Ø¯.')

async def setconcurrency(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global current_concurrency
    if context.args:
        try:
            concurrency = int(context.args[0])
            if CONFIG["MIN_CONCURRENCY"] <= concurrency <= CONFIG["MAX_CONCURRENCY"]:
                with lock:
                    current_concurrency = concurrency
                await update.message.reply_text(f'ØªÙ… ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ²Ø§Ù…Ù† Ø¥Ù„Ù‰: {concurrency}')
            else:
                await update.message.reply_text(f'ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„ØªØ²Ø§Ù…Ù† Ø¨ÙŠÙ† {CONFIG["MIN_CONCURRENCY"]} Ùˆ {CONFIG["MAX_CONCURRENCY"]}.')
        except ValueError:
            await update.message.reply_text('Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙ‚Ø¯ÙŠÙ… Ø±Ù‚Ù… ØµØ­ÙŠØ­ Ù„Ù„ØªØ²Ø§Ù…Ù†.')
    else:
        await update.message.reply_text('Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙ‚Ø¯ÙŠÙ… Ù‚ÙŠÙ…Ø© Ù„Ù„ØªØ²Ø§Ù…Ù†. Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: /setconcurrency <number>')

async def settarget(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global current_visits_per_day
    if context.args:
        try:
            target = int(context.args[0])
            if target > 0:
                with lock:
                    current_visits_per_day = target
                await update.message.reply_text(f'ØªÙ… ØªØ¹ÙŠÙŠÙ† Ù‡Ø¯Ù Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠ Ø¥Ù„Ù‰: {target}')
            else:
                await update.message.reply_text('ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù‡Ø¯Ù Ø±Ù‚Ù…Ù‹Ø§ Ù…ÙˆØ¬Ø¨Ù‹Ø§.')
        except ValueError:
            await update.message.reply_text('Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙ‚Ø¯ÙŠÙ… Ø±Ù‚Ù… ØµØ­ÙŠØ­ Ù„Ù„Ù‡Ø¯Ù.')
    else:
        await update.message.reply_text('Ø§Ù„Ø±Ø¬Ø§Ø¡ ØªÙ‚Ø¯ÙŠÙ… Ù‚ÙŠÙ…Ø© Ù„Ù„Ù‡Ø¯Ù. Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: /settarget <number>')

async def sysinfo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    cpu_percent = psutil.cpu_percent(interval=1)
    cpu_count = psutil.cpu_count()
    cpu_freq = psutil.cpu_freq()
    mem = psutil.virtual_memory()
    swap = psutil.swap_memory()
    disk = psutil.disk_usage('/')
    boot_time = time.ctime(psutil.boot_time())

    response = (
        f"ğŸ–¥ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:\n"
        f"------------------
        #pip install requests psutil numpy scikit-learn python-telegram-bot geopy beautifulsoup4 torch optuna
